{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import product\n",
    "import cv2\n",
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "import chromadb\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du client et de la base locale\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a collection in ChromaDB\n",
    "def create_collection(collection_name, embedding_function):\n",
    "    \"\"\"\n",
    "    Cr√©e une collection dans ChromaDB avec la fonction d'embedding sp√©cifi√©e.\n",
    "    \n",
    "    Args:\n",
    "        collection_name (str): Le nom de la collection √† cr√©er.\n",
    "        embedding_function: La fonction d'embedding √† utiliser pour la collection.\n",
    "    \n",
    "    Returns:\n",
    "        Collection: La collection cr√©√©e.\n",
    "    \"\"\"\n",
    "    return client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de documents avec vecteurs + m√©tadonn√©es\n",
    "def add_detection(collection, ids, embeddings, metadatas):\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "\n",
    "def del_detection(collection, ids):\n",
    "    collection.delete(ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charge le mod√®le CLIP et le pr√©processeur\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path \n",
    "path = \"C:/Users/cleme/Desktop/Ecole/M1/Projet/Projet_S2/V2/Video_test\"\n",
    "video_name = \"This Video Is 3 Seconds\"\n",
    "video_path = os.path.join(path, video_name)+\".mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize the video_name to make it a valid collection name\n",
    "sanitized_video_name = video_name.replace(\" \", \"_\")\n",
    "sanitized_video_name = sanitized_video_name.replace(\"-\", \"_\")\n",
    "sanitized_video_name = sanitized_video_name.replace(\".\", \"_\")\n",
    "\n",
    "# Define a custom embedding function that conforms to the required signature\n",
    "class ClipEmbeddingFunction:\n",
    "\tdef __call__(self, input):\n",
    "\t\t# Ensure input is a tensor and process it using the CLIP model\n",
    "\t\tif not isinstance(input, torch.Tensor):\n",
    "\t\t\traise ValueError(\"Input to embedding function must be a torch.Tensor\")\n",
    "\t\treturn clip_model.encode_image(input)\n",
    "\n",
    "# Create the collection with the custom embedding function\n",
    "embedding_function = ClipEmbeddingFunction()\n",
    "create_collection(sanitized_video_name, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas besoin de redimensionner les frames, CLIP le fait tout seul gr√¢ce √† sa fonction \"preprocess\". Faire 2 redimensionnages augmente simplement le temps de calcul, voire peut faire perdre des informations de l'image d'origine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS_TARGET = 24  # target FPS\n",
    "\n",
    "# Clear cache and re-download the SentenceTransformer model\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "\n",
    "def create_unique_folder(base_dir, prefix=\"frames\"):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%d%m_%H%M%S\")\n",
    "    folder_path = os.path.join(base_dir, f\"{prefix}_{timestamp}\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    return folder_path\n",
    "\n",
    "def extract_frames(video_path, output_dir, target_fps, start_frame=0, custom_output_dir=None):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video file '{video_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video '{video_path}'\")\n",
    "        return None\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames_original = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total_frames_target = int((total_frames_original / original_fps) * target_fps)\n",
    "\n",
    "    print(f\"Original total frames: {total_frames_original}\")\n",
    "    print(f\"Target total frames: {total_frames_target}\")\n",
    "\n",
    "    # Check and adjust start_frame\n",
    "    if start_frame > total_frames_target:\n",
    "        print(f\"Error: Start frame ({start_frame}) is greater than total target frames ({total_frames_target}).\")\n",
    "        start_frame = total_frames_target\n",
    "        print(f\"start_frame adjusted to {start_frame}\")\n",
    "\n",
    "    # Cross multiplication to adjust start time\n",
    "    start_time = (start_frame * total_frames_original) / total_frames_target\n",
    "    adjusted_start_frame = round(start_time)\n",
    "    print(f\"Adjusted start frame: {adjusted_start_frame}\")\n",
    "\n",
    "    # Use custom directory or create a new one\n",
    "    frames_dir = custom_output_dir if custom_output_dir else create_unique_folder(output_dir, \"frames_resized\")\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    # Extract and resize frames starting from the adjusted frame\n",
    "    extracted_count = start_frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, adjusted_start_frame)\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        frame_filename = os.path.join(frames_dir, f\"frame_{extracted_count:06d}.png\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        extracted_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extraction and resizing completed. {extracted_count - adjusted_start_frame} images saved to '{frames_dir}'\")\n",
    "    return frames_dir\n",
    "\n",
    "def create_video_from_frames(frames_dir, output_path, fps):\n",
    "    frames = sorted(f for f in os.listdir(frames_dir) if f.endswith(\".png\"))\n",
    "    if not frames:\n",
    "        print(f\"Error: No frames found in '{frames_dir}'\")\n",
    "        return\n",
    "\n",
    "    first_frame = cv2.imread(os.path.join(frames_dir, frames[0]))\n",
    "    height, width, _ = first_frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame_filename in frames:\n",
    "        frame = cv2.imread(os.path.join(frames_dir, frame_filename))\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video saved to '{output_path}'\")\n",
    "\n",
    "# ============================\n",
    "#           EXECUTION\n",
    "# ============================\n",
    "\n",
    "path = \"C:/Users/cleme/Desktop/Ecole/M1/Projet/Projet_S2/V2/Video_test\"\n",
    "video_name = \"This Video Is 3 Seconds\"\n",
    "video_path = os.path.join(path, video_name)+\".mp4\"\n",
    "\n",
    "frames_base_dir = path + \"/extraction\"\n",
    "timestamp = datetime.datetime.now().strftime(\"%d%m_%H%M%S\")\n",
    "\n",
    "custom_folder = \"\"\n",
    "frames_dir = extract_frames(video_path, frames_base_dir, FPS_TARGET, start_frame=0, custom_output_dir=custom_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingComparator:\n",
    "\n",
    "    # Constructor to initialize the CLIP model and processor (options: \"openai/clip-vit-base-patch32\" for speed, \"openai/clip-vit-large-patch14\" for accuracy).\n",
    "\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    # Encode an image into an embedding vector (returns a numpy array of shape (1, embedding_dim)).\n",
    "        \n",
    "    def encode_image(self, image_path):\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.get_image_features(**inputs)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        image_embedding = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        return image_embedding.cpu().numpy()\n",
    "    \n",
    "    # Encode text into an embedding vector (returns a numpy array of shape (1, embedding_dim)).\n",
    "\n",
    "    def encode_text(self, text):\n",
    "       \n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        text_embedding = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_embedding.cpu().numpy()\n",
    "    \n",
    "    # Compute cosine similarity between two embedding vectors (returns a score between -1 and 1).\n",
    "    \n",
    "    def compare_embeddings(self, embedding1, embedding2):\n",
    "        \n",
    "        return cosine_similarity(embedding1, embedding2)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of use : \n",
    "\n",
    "def testing(image,text) :\n",
    "    comparator = EmbeddingComparator()\n",
    "    image_embedding = comparator.encode_image(image) \n",
    "    text_embedding = comparator.encode_text(text)\n",
    "    \n",
    "    similarity = comparator.compare_embeddings(image_embedding, text_embedding)\n",
    "    print(f\"Similarity between image and '{text}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcourir chaque image dans le dossier frames_base_dir\n",
    "for root, dirs, files in os.walk(frames_base_dir):\n",
    "    comparator = EmbeddingComparator()\n",
    "    for file in files:\n",
    "        if file.endswith(('.png', '.jpg', '.jpeg')):  # V√©rifier les extensions d'image\n",
    "            image_path = os.path.join(root, file)\n",
    "            \n",
    "            # Extraire l'embedding directement √† partir du chemin de l'image\n",
    "            embedding = comparator.encode_image(image_path)\n",
    "            \n",
    "            # Ensure embedding is a PyTorch tensor before calling detach()\n",
    "            if isinstance(embedding, np.ndarray):\n",
    "                embedding = torch.tensor(embedding)\n",
    "\n",
    "            print(f\"Embedding extrait pour {file}: {embedding.detach().cpu().numpy()}\")\n",
    "\n",
    "            # Ajouter l'embedding √† la collection ChromaDB\n",
    "            add_detection(\n",
    "                collection=client.get_collection(sanitized_video_name),\n",
    "                ids=[file],\n",
    "                embeddings=embedding.detach().cpu().numpy().tolist(),\n",
    "                metadatas=[{\"frame\": file}]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage de chaque nom de collection\n",
    "collections = client.list_collections()\n",
    "print(\"Collections existantes :\")\n",
    "for collection in collections:\n",
    "    print(f\"- {collection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©rer toutes les donn√©es (ids, embeddings, metadatas) de chaque collection\n",
    "\n",
    "collections = client.list_collections() # Liste des collections\n",
    "\n",
    "for collection in collections:\n",
    "    print(f\"Collection: {collection}\")\n",
    "    detection = client.get_collection(collection) # R√©cup√©rer la collection\n",
    "    all_data = detection.get(include=[\"embeddings\", \"metadatas\"]) # R√©cup√©rer toutes les donn√©es de la collection\n",
    "\n",
    "    # Afficher tout le contenu\n",
    "    for i in range(len(all_data[\"ids\"])):\n",
    "        print(f\"üîπ ID: {all_data['ids'][i]}\")\n",
    "        if all_data['embeddings'] is not None:\n",
    "            print(f\"üß† Embedding: {all_data['embeddings'][i]}\")\n",
    "        print(f\"üìå M√©tadonn√©es: {all_data['metadatas'][i]}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\"\"\"\n",
    "print(len(all_data['ids']))\n",
    "print(all_data['embeddings'][0].shape)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Norm:\", torch.norm(embedding).item())               # ‚úÖ ‚âà 1.0\n",
    "print(\"Mean:\", embedding.mean().item())                    # ‚úÖ ‚âà 0.0\n",
    "print(\"Std:\", embedding.std().item())                      # ‚úÖ ‚âà 0.05\n",
    "print(\"Min/Max:\", embedding.min().item(), embedding.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©rer une requ√™te textuelle et g√©n√©rer son embedding\n",
    "def get_text_embedding(query):\n",
    "    comparator = EmbeddingComparator()\n",
    "    text_embedding = comparator.encode_text(query)\n",
    "    return text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_images(frames_dir, query_embedding, similarity_threshold, top_x):\n",
    "    # Dossiers de sortie\n",
    "    above_threshold_dir = os.path.join(frames_dir, \"above_threshold\")\n",
    "    top_x_dir = os.path.join(frames_dir, \"top_x_similar\")\n",
    "    os.makedirs(above_threshold_dir, exist_ok=True)\n",
    "    os.makedirs(top_x_dir, exist_ok=True)\n",
    "\n",
    "    # R√©cup√©rer la collection\n",
    "    collection = client.get_collection(sanitized_video_name)\n",
    "    all_data = collection.get(include=[\"documents\", \"embeddings\", \"metadatas\"])\n",
    "\n",
    "    # Calculer les similarit√©s\n",
    "    similarities = []\n",
    "    for i in range(len(all_data[\"ids\"])):\n",
    "        image_id = all_data[\"ids\"][i]\n",
    "        image_embedding = np.array(all_data[\"embeddings\"][i])\n",
    "        similarity = cosine_similarity(query_embedding, image_embedding.reshape(1, -1))[0][0]\n",
    "        similarities.append((image_id, similarity))\n",
    "    \n",
    "    # Copier les images au-dessus du seuil\n",
    "    for image_id, similarity in similarities:\n",
    "        if similarity >= similarity_threshold:\n",
    "            src_path = os.path.join(frames_dir, image_id)\n",
    "            dst_path = os.path.join(above_threshold_dir, image_id)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "    # Trier les similarit√©s par ordre d√©croissant\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Copier les x images les plus similaires\n",
    "    for i, (image_id, similarity) in enumerate(similarities[:top_x]):\n",
    "        src_path = os.path.join(frames_dir, image_id)\n",
    "        dst_path = os.path.join(top_x_dir, f\"{i+1:02d}_{image_id}\")\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "    print(f\"Images au-dessus du seuil ({similarity_threshold}) copi√©es dans '{above_threshold_dir}'\")\n",
    "    print(f\"Top {top_x} images les plus similaires copi√©es dans '{top_x_dir}'\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "query = \"Person walking in a park\"\n",
    "query_embedding = get_text_embedding(query)\n",
    "find_similar_images(frames_dir, query_embedding, similarity_threshold=0.18, top_x=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du dossier \"above_threshold\"\n",
    "above_threshold_dir = os.path.join(frames_dir, \"above_threshold\")\n",
    "\n",
    "# Chemin de sortie pour la vid√©o recr√©√©e\n",
    "output_video_path = os.path.join(frames_dir, \"above_threshold_video.mp4\")\n",
    "\n",
    "# Recr√©er la vid√©o √† partir des images\n",
    "create_video_from_frames(above_threshold_dir, output_video_path, FPS_TARGET)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VGS_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
