<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 10px;
        }
        h1 {
            font-size: 24px;
            margin-bottom: 10px;
            text-align: center;
        }
        h2 {
            font-size: 20px;
            margin-bottom: 8px;
        }
        ul {
            margin: 10px 0;
            padding-left: 20px;
        }
        li {
            margin-bottom: 5px;
        }
        a {
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>CLIP Analysis in Generative Surveillance</h1>

    <p>Welcome to the <strong>CLIP Analysis</strong> module of our application. This document explains the functionality of the CLIP model and how it is utilized in our software for advanced image and text analysis.</p>
    
    <h2>What is CLIP?</h2>
    <p>
        CLIP (Contrastive Language‚ÄìImage Pretraining) is a model developed by OpenAI that connects images and text, enabling multimodal analysis. It can understand the relationship between visual and textual information, making it useful for tasks such as:
    </p>
    <ul>
        <li>Image captioning</li>
        <li>Visual question answering</li>
        <li>Image-text similarity analysis</li>
    </ul>
    <p>
        CLIP works by encoding both images and text into a shared embedding space, where the similarity between embeddings can be calculated using metrics like cosine similarity.
    </p>

    <h2>How We Use CLIP in Our Application</h2>
    <p>
        In our application, CLIP is used to analyze and compare embeddings of images and text prompts. This allows us to perform tasks such as:
    </p>
    <ul>
        <li>Finding the most relevant images for a given text prompt.</li>
        <li>Filtering images based on a similarity threshold.</li>
        <li>Organizing and analyzing collections of images based on their semantic content.</li>
    </ul>

    <h3>Workflow</h3>
    <ol>
        <li><strong>Image Encoding:</strong> Images are encoded into normalized embeddings using the CLIP model and stored in the Bank.</li>
        <li><strong>Text Encoding:</strong> Text prompts are similarly encoded into embeddings.</li>
        <li><strong>Similarity Calculation:</strong> The cosine similarity between image and text embeddings is calculated to determine their relevance.</li>
        <li><strong>Result Organization:</strong> Results are organized into collections, with options to filter by similarity thresholds.</li>
    </ol>

    <h3>Example Use Case</h3>
    <p>
        Suppose you have a collection of images and want to find the ones most relevant to the prompt "sunset over the ocean." Our application will:
    </p>
    <ul>
        <li>Click on the <strong>‚ûï</strong> button to add videos or images to the Bank.</li>
        <li>The <strong>üè¶</strong> show in your logs the collections in your database, and the amount of frames embedded.</li>
        <li>Click on the <strong>üßπ</strong> to reset the data base.</li>
        <li>Use the <strong>‚úèÔ∏è</strong> button to rename files, <strong>üì§</strong> to export, or <strong>üóëÔ∏è</strong> to delete files.</li>
        <li>Once files are added to the Bank, enter a query in the prompt input field and click on <strong>Start Analysis</strong>.</li>
        <li>The application will encode the prompt into a text embedding, compare it with the embeddings of all images in the collection, and return results where the similarity is above a threshold.</li>
    </ul>

    <h2>File Hierarchy</h2>
    <p>The following is a description of the files and folders in the software:</p>
    <ul>
        <li><strong>ui.py</strong> & <strong>utils.py</strong>: Initialize the UI and contain navigation functions such as delete, add, remove, rename files, display files, toggle navigation, etc.</li>
        <li><strong>imports.py</strong>: List of imports used across other scripts.</li>
        <li><strong>VideoReader.py</strong>: Functions to read, play, pause, and manage video media.</li>
        <li><strong>upload.py</strong>: Script to upload a video, extract frames, and generate their embeddings.</li>
        <li><strong>Clip_Analysis.py</strong>: Script for embedding text and performing analysis.</li>
        <li><strong>faiss_instance.py</strong> & <strong>faissClient.py</strong>: Functions related to the vector database using FAISS.</li>
        <li><strong>data.faiss</strong> & <strong>data_metadata.json</strong>: The vector database containing embeddings and its metadata file.</li>
        <li><strong>threads.py</strong>: Handles uploads and analysis through threads, allowing the software to remain usable while tasks are being performed.</li>
        <li><strong>Bank folder</strong>: Contains original videos to analyze and their extracted frames.</li>
        <li><strong>Analysis folder</strong>: Contains CLIP analysis results for user requests.</li>
        <li><strong>Asset folder</strong>: Contains assets such as the logo (.png, .svg, .ico), loading GIFs, and a list of emojis used in the code.</li>
    </ul>

    <h2>Technical Details</h2>
    <p>
        The CLIP model used in our application is <strong>openai/clip-vit-base-patch32</strong>. It is loaded and executed on either a GPU (if available) or a CPU. The embeddings are normalized to ensure consistent similarity calculations.
    </p>
    <p>
        Results are saved in structured folders, with detailed logs and metadata for further analysis. The application also integrates with FAISS for efficient nearest-neighbor searches on large datasets.
    </p>

    <h2>Documentation</h2>
    <p>For more information on CLIP, refer to the official documentation:</p>
    <ul>
        <li><a href="https://github.com/openai/CLIP?tab=readme-ov-file">CLIP Documentation</a></li>
    </ul>
</body>
</html>