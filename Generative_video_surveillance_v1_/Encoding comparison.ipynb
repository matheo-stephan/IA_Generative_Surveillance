{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers pillow scikit-learn numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingComparator:\n",
    "\n",
    "    # Constructor to initialize the CLIP model and processor (options: \"openai/clip-vit-base-patch32\" for speed, \"openai/clip-vit-large-patch14\" for accuracy).\n",
    "\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    # Encode an image into an embedding vector (returns a numpy array of shape (1, embedding_dim)).\n",
    "        \n",
    "    def encode_image(self, image_path):\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.get_image_features(**inputs)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        image_embedding = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        return image_embedding.cpu().numpy()\n",
    "    \n",
    "    # Encode text into an embedding vector (returns a numpy array of shape (1, embedding_dim)).\n",
    "\n",
    "    def encode_text(self, text):\n",
    "       \n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        text_embedding = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_embedding.cpu().numpy()\n",
    "    \n",
    "    # Compute cosine similarity between two embedding vectors (returns a score between -1 and 1).\n",
    "    \n",
    "    def compare_embeddings(self, embedding1, embedding2):\n",
    "        \n",
    "        return cosine_similarity(embedding1, embedding2)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(imagepath, simple_text, detailed_text, different_text, random_text):\n",
    "\n",
    "    image_path = imagepath\n",
    "    comparator = EmbeddingComparator()\n",
    "    image_embedding = comparator.encode_image(image_path) \n",
    "\n",
    "    # Example 1: Compare image with text \n",
    "    text = simple_text\n",
    "    text_embedding = comparator.encode_text(text)\n",
    "    similarity = comparator.compare_embeddings(image_embedding, text_embedding)\n",
    "    print(f\"Similarity between image and '{text}': {similarity:.4f}\")\n",
    "\n",
    "    # Example 2: Compare with another text\n",
    "    text = detailed_text\n",
    "    text_embedding = comparator.encode_text(text)\n",
    "    similarity = comparator.compare_embeddings(image_embedding, text_embedding)\n",
    "    print(f\"Similarity between image and '{text}': {similarity:.4f}\")\n",
    "\n",
    "    # Example 3: Compare with another text\n",
    "    text = different_text\n",
    "    text_embedding = comparator.encode_text(text)\n",
    "    similarity = comparator.compare_embeddings(image_embedding, text_embedding)\n",
    "    print(f\"Similarity between image and '{text}': {similarity:.4f}\")\n",
    "\n",
    "    # Example 4: Compare with random text\n",
    "    text = random_text\n",
    "    text_embedding = comparator.encode_text(text)\n",
    "    similarity = comparator.compare_embeddings(image_embedding, text_embedding)\n",
    "    print(f\"Similarity between image and '{text}': {similarity:.4f}\")\n",
    "    print(f\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------BASIC TEST-------\n",
      "\n",
      "Similarity between the image and itself: 1.0000\n",
      "Similarity between the text and itself: 1.0000\n",
      "\n",
      "-------SIMPLE IMAGE OF A DOG-------\n",
      "\n",
      "Similarity between image and 'a picture of a dog': 0.2776\n",
      "Similarity between image and 'a picture of a golden retriever sticking its tongue out': 0.3076\n",
      "Similarity between image and 'a cat': 0.1926\n",
      "Similarity between image and '??dasd;'12p[]': 0.2000\n",
      "\n",
      "\n",
      "-------IMAGE OF A DOG AND A BALL-------\n",
      "\n",
      "Similarity between image and 'a picture of a dog and a ball': 0.2975\n",
      "Similarity between image and 'a picture of a small brown dog and a yellow ball': 0.3001\n",
      "Similarity between image and 'a cat': 0.1954\n",
      "Similarity between image and '??dasd;'12p[]': 0.2089\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic testing of the comparator to check if it works as expected (should return 1.0 for identical inputs)\n",
    "print(f\"-------BASIC TEST-------\"); print(f\"\")\n",
    "comparator = EmbeddingComparator();\n",
    "embedding1 = comparator.encode_image(\"dog.jpg\"); \n",
    "embedding2 = comparator.encode_image(\"dog.jpg\");\n",
    "similarity = comparator.compare_embeddings(embedding1, embedding2);\n",
    "print(f\"Similarity between the image and itself: {similarity:.4f}\")\n",
    "embedding1 = comparator.encode_text(\"dog\");\n",
    "embedding2 = comparator.encode_text(\"dog\");\n",
    "similarity = comparator.compare_embeddings(embedding1, embedding2);   \n",
    "print(f\"Similarity between the text and itself: {similarity:.4f}\")\n",
    "print(f\"\")  \n",
    "\n",
    "\n",
    "# First we will use the simple image of a dog\n",
    "print(f\"-------SIMPLE IMAGE OF A DOG-------\"); print(f\"\")\n",
    "testing(\"dog.jpg\", \"a picture of a dog\", \"a picture of a golden retriever sticking its tongue out\", \"a cat\", \"??dasd;'12p[]\")\n",
    "print(f\"\")\n",
    "\n",
    "# Next we will use the picture of a dog and a ball\n",
    "print(f\"-------IMAGE OF A DOG AND A BALL-------\"); print(f\"\")\n",
    "testing(\"dog_ball.jpg\", \"a picture of a dog and a ball\", \"a picture of a small brown dog and a yellow ball\", \"a cat\", \"??dasd;'12p[]\")\n",
    "print(f\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
