# V2 avec CLIP

---

# ğŸ§© Objectif du Projet

**Ce projet a pour objectif de mettre en place un benchmark vidÃ©o basÃ© sur le dataset UCF Crimes, permettant :**

- Lâ€™extraction et le redimensionnement des frames dâ€™une vidÃ©o.
- La gÃ©nÃ©ration dâ€™un embedding vectoriel pour chaque frame Ã  lâ€™aide dâ€™un modÃ¨le CLIP.
- Lâ€™Ã©valuation de robustesse via ajout de bruit ou de modifications artificielles. (optionnelle)
- La gÃ©nÃ©ration d'un embedding textuel.
- Ajout des vecteurs dans une BDD vectiorelle.
- La recherche via similaritÃ© entre un texte et les images extraites.
- Triage par seuil de similaritÃ© ou par similaritÃ© dÃ©croissante.
- Extraction des frames selectionnÃ©es.
- Creation d'une nouvelle vidÃ©o Ã  partir des frames extraites.

---

# âš™ï¸ DÃ©pendances

**Les bibliothÃ¨ques suivantes sont nÃ©cessaires au bon fonctionnement :**

- opencv-python
- torch
- transformers
- sentence-transformers
- chromadb
- numpy
- Pillow
- shutil
- datetime
- os
- cv2
- clip

---

# ğŸ“‚ Structure Principale du Projet

## ğŸ”§ 1. ParamÃ¨tres de configuration
```python
FPS_TARGET = 24   # Frame rate cible
TARGET_SIZE = (640, 360)  # Dimensions de sortie des frames
```
## ğŸ“ 2. Extraction et redimensionnement des frames

**Fonctions :**

- `extract_and_resize_frames(...)`: extrait les frames dâ€™une vidÃ©o, les redimensionne, et les stocke dans un dossier.
- `resize_frame(...)`: resize une frame unique Ã  la taille cible.

**ğŸ” Points techniques :**

- Gestion automatique des FPS et adaptation de start_frame.
- CrÃ©ation dâ€™un rÃ©pertoire unique pour stocker les frames (via `create_unique_folder`).

## ğŸ“¹ 3. GÃ©nÃ©ration vidÃ©o Ã  partir de frames
```python
create_video_from_frames(frames_dir, output_path, fps)
```
Permet de recrÃ©er une vidÃ©o Ã  partir des images traitÃ©es.

---

# ğŸ¤– Encodage et SimilaritÃ©

## ğŸ’¡ Encodage

- Utilisation du modÃ¨le OpenAI CLIP (`openai/clip-vit-base-patch32`) pour produire des embeddings vectoriels Ã  partir dâ€™images et de textes.
- Normalisation des vecteurs (`L2 norm`) aprÃ¨s encodage.

## ğŸ” Recherche sÃ©mantique

```python
find_similar_images(frames_dir, query_embedding, similarity_threshold, top_x)
```
Calcule la similaritÃ© entre lâ€™embedding texte (`query_embedding`) et ceux de chaque image dans la base de donnÃ©es vectorielle (`ChromaDB`).

**Deux critÃ¨res :**

- Images au-dessus du seuil (`similarity_threshold`)
- Top X images les plus similaires

**âœ… Le systÃ¨me adapte dynamiquement le seuil si demandÃ©, en retirant 10% Ã  la similaritÃ© maximale dÃ©tectÃ©e.**

## ğŸ“ SimilaritÃ©

Le systÃ¨me utilise **la similaritÃ© cosinus** pour comparer les vecteurs d'embedding d'une image et d'un texte.

### ğŸ” Qu'est-ce que la similaritÃ© cosinus ?

La similaritÃ© cosinus mesure **lâ€™angle** entre deux vecteurs dans un espace vectoriel. Elle ne prend pas en compte la norme (la taille) des vecteurs, mais seulement leur direction.

Elle est calculÃ©e avec la formule suivante :
```python
similarity(A, B) = (A â‹… B) / (||A|| * ||B||)
```
- `A â‹… B` est le produit scalaire des deux vecteurs.
- `||A||` et `||B||` sont les normes (longueurs) des vecteurs.

### ğŸ¯ InterprÃ©tation des valeurs

- **1.0** â†’ les vecteurs pointent exactement dans la mÃªme direction (sÃ©mantiquement identiques).
- **0.0** â†’ les vecteurs sont orthogonaux (aucune similaritÃ©).
- **< 0** â†’ vecteurs opposÃ©s (sans lien), mais rarement observÃ© ici car les vecteurs sont gÃ©nÃ©ralement **positifs** et **normalisÃ©s**.

### ğŸ§  Pourquoi la normalisation ?

Les embeddings sont souvent **L2-normalisÃ©s**, câ€™est-Ã -dire que leur norme est ramenÃ©e Ã  1 :

```python
embedding = embedding / embedding.norm(dim=-1, keepdim=True)
```
Cela permet que la similaritÃ© cosinus soit simplement le produit scalaire entre deux vecteurs de norme unitaire.

### ğŸ“Š Utilisation dans le projet
Dans ce projet, on compare un embedding texte Ã  des embeddings image :
```python
similarity = cosine_similarity(text_embedding, image_embedding)[0][0]
```

**Puis :**

- On trie les rÃ©sultats par ordre dÃ©croissant de similaritÃ©.
- On garde ceux au-dessus dâ€™un seuil dynamique (90% de la similaritÃ© maximale observÃ©e).
- On sauvegarde les X meilleurs rÃ©sultats dans un dossier dÃ©diÃ©.
  
---

# ğŸ§ª Tests de Robustesse

**Fonction intÃ©grÃ©e :**

```python
add_salt_pepper_noise(image, amount=0.05)
```
- Ajoute un bruit de type "sel et poivre" (ajout alÃ©atoire de pixel blanc ou noir) Ã  une image.
- Permet de tester la robustesse de la reconnaissance visuelle par similaritÃ©.

---

# ğŸ’¾ Stockage vectoriel avec ChromaDB

Ce projet utilise **ChromaDB** comme base de donnÃ©es vectorielle pour indexer et rechercher efficacement les embeddings des images extraites depuis des vidÃ©os.

## ğŸ§  Qu'est-ce qu'une base de donnÃ©es vectorielle ?

Une base de donnÃ©es vectorielle permet de stocker des **vecteurs d'embedding** et d'effectuer des recherches de similaritÃ© entre eux de maniÃ¨re rapide et scalable. Contrairement aux bases de donnÃ©es classiques qui indexent des textes ou des valeurs numÃ©riques simples, ici ce sont des vecteurs Ã  haute dimension (souvent 768 dimensions) qui sont stockÃ©s.

Chaque vecteur reprÃ©sente la **sÃ©mantique dâ€™une image** (ou dâ€™un texte) gÃ©nÃ©rÃ©e par un modÃ¨le de type CLIP.

## ğŸ—ï¸ Structure de ChromaDB

Chaque vecteur est stockÃ© dans une **collection**. Une collection contient :
- `id` : un identifiant unique (ex. nom de frame)
- `embedding` : le vecteur numÃ©rique reprÃ©sentant lâ€™image
- `document` *(optionnel)* : texte associÃ© (non utilisÃ© ici)
- `metadata` : informations supplÃ©mentaires (par exemple le nom de la vidÃ©o, l'index de frame, etc.)

Exemple dâ€™ajout dâ€™un vecteur :
```python
collection.add(
    ids=["frame_000012.png"],
    embeddings=[image_embedding],
    metadatas=[{"video_name": "video_demo", "frame_index": 12}]
)
```
### ğŸ” Recherche par similaritÃ©

Dans ce projet, une fois les embeddings texte et image extraits et stockÃ©s dans ChromaDB, nous utilisons la **similaritÃ© cosinus** pour retrouver les images les plus proches d'une requÃªte textuelle. La similaritÃ© cosinus mesure l'angle entre deux vecteurs dans l'espace, et varie entre -1 (opposÃ©s) et 1 (identiques).

#### Deux approches sont utilisÃ©es pour filtrer les rÃ©sultats :

#### ğŸ§ª 1. Recherche `top_x` (meilleures similaritÃ©s)
Cette mÃ©thode consiste Ã  rÃ©cupÃ©rer les **X images les plus proches** de la requÃªte, peu importe leur score exact.

**Utilisation typique :**
```python
top_x_similar = sorted(similarities, key=lambda x: x["similarity"], reverse=True)[:top_x] 
```
Les X images avec la similaritÃ© la plus Ã©levÃ©e sont ensuite copiÃ©es dans un dossier top_x_similar pour Ãªtre visualisÃ©es.

#### ğŸ“ 2. Recherche above_threshold (filtrage par seuil)
Cette mÃ©thode consiste Ã  ne garder que les images dont la similaritÃ© dÃ©passe une certaine valeur seuil.

Le seuil peut Ãªtre fixÃ© manuellement (similarity_threshold = 0.18), ou dÃ©fini dynamiquement comme suit :
```python
max_similarity = max(similarities, key=lambda x: x["similarity"])["similarity"]
similarity_threshold = max_similarity * 0.9 # 90% du maximum
```
Seules les images ayant une similaritÃ© supÃ©rieure ou Ã©gale Ã  ce seuil sont conservÃ©es et stockÃ©es dans un dossier above_threshold.

## ğŸ“ Organisation par vidÃ©o

Chaque vidÃ©o analysÃ©e est liÃ©e Ã  une collection diffÃ©rente, identifiÃ©e via son nom (souvent `sanitized_video_name`). Cela permet de sÃ©parer les rÃ©sultats par vidÃ©o et dâ€™Ã©viter les collisions.

## ğŸ¯ IntÃ©gration dans le pipeline

**Voici comment ChromaDB est utilisÃ© dans le programme :**

- Ã€ chaque extraction de frame â†’ lâ€™image est encodÃ©e en embedding.
- Lâ€™embedding est ajoutÃ© Ã  la collection correspondant Ã  la vidÃ©o.
- Lors de la recherche, lâ€™embedding du texte est comparÃ© Ã  tous ceux de la collection.
- Les rÃ©sultats sont triÃ©s, et les meilleures images sont copiÃ©es dans des dossiers (`top_x_similar`, `above_threshold`).

--- 

# ğŸ”„ Pipeline Complet

1. Charger une vidÃ©o
2. Extraire & redimensionner les frames
3. Encoder chaque frame (image â†’ vecteur)
4. InsÃ©rer les embeddings dans ChromaDB
5. Encoder la requÃªte (texte â†’ vecteur)
6. Calcul des similaritÃ©s (cosine)
7. Filtrage des images (top-k ou seuil dynamique)
8. Reconstruction dâ€™une vidÃ©o ou affichage

--- 

# ğŸš€ Guide dâ€™utilisation du programme

Cette section dÃ©crit comment utiliser le systÃ¨me Ã©tape par Ã©tape, depuis lâ€™entrÃ©e dâ€™une vidÃ©o jusquâ€™Ã  lâ€™obtention des rÃ©sultats de similaritÃ©.

## 1. ğŸ“¥ PrÃ©parer votre vidÃ©o

Placez votre vidÃ©o dans un dossier accessible, par exemple :
`video_path = "path/to/your/video.mp4"`

## 2. ğŸ–¼ï¸ Extraire et redimensionner les frames

Appelez la fonction suivante :
```python
frames_dir = extract_and_resize_frames(
    video_path=video_path,
    output_dir="path/to/output/frames",
    target_fps=FPS_TARGET,
    target_size=TARGET_SIZE,
    start_frame=0  # optionnel
)
```
Cela extrait les frames, les redimensionne et les stocke dans un dossier.

## 3. ğŸ” Encoder les images extraites

Parcourez le dossier de frames pour encoder chaque image via CLIP :
```python
for frame in os.listdir(frames_dir):
    image_path = os.path.join(frames_dir, frame)
    embedding = encode_image(image_path)  # ou via votre classe
    # InsÃ©rez dans ChromaDB avec image_id = nom du fichier
```

## 4. ğŸ’¬ Entrer une requÃªte texte

Encodez votre requÃªte :
```python
query = "Person walking in a park"
query_embedding = get_text_embedding(query)
```

## 5. ğŸ§  Trouver les images similaires

Appelez la fonction de comparaison :
```python
find_similar_images(
    frames_dir=frames_dir,
    query_embedding=query_embedding,
    similarity_threshold=None,  # auto-threshold si None
    top_x=50
)
```
Cela copie :

- Les images au-dessus du seuil calculÃ© dynamiquement.
- Les Top X plus similaires dans deux sous-dossiers distincts.

## 6. ğŸ¥ RecrÃ©er une vidÃ©o (optionnel)

Vous pouvez ensuite recompiler les frames similaires en une vidÃ©o :
```python
create_video_from_frames(
    frames_dir="path/to/frames/analysed_frames",
    output_path="path/to/output_video.mp4",
    fps=FPS_TARGET
)
```
